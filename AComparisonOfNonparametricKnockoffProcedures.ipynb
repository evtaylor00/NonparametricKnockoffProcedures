{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ace4633a-a129-4654-8031-13dcd20f6695",
   "metadata": {},
   "source": [
    "# STAT 6385: Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af763f05-7abb-4543-94ba-1f2586f02205",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c97419c-39dd-44c6-85a8-c95d1ad56fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import roc_auc_score, pairwise_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import ks_2samp, chi2_contingency\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import List, Tuple, Union\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d424c811-abe0-4d48-a7f2-a4f1137f429f",
   "metadata": {},
   "source": [
    "## Knockoff Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3040b6-3986-490d-b9b5-03c6b12568ea",
   "metadata": {},
   "source": [
    "### Deep Knockoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d9f6640-2315-4202-bee3-101f5b0982a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code downloaded from https://github.com/msesia/deepknockoffs/tree/master/DeepKnockoffs\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "min_var_est = 1e-8\n",
    "\n",
    "def linear_mmd2(f_of_X, f_of_Y):\n",
    "    loss = 0.0\n",
    "    delta = f_of_X - f_of_Y\n",
    "    loss = torch.mean((delta[:-1] * delta[1:]).sum(1))\n",
    "    return loss\n",
    "\n",
    "def poly_mmd2(f_of_X, f_of_Y, d=2, alpha=1.0, c=2.0):\n",
    "    K_XX = (alpha * (f_of_X[:-1] * f_of_X[1:]).sum(1) + c)\n",
    "    K_XX_mean = torch.mean(K_XX.pow(d))\n",
    "\n",
    "    K_YY = (alpha * (f_of_Y[:-1] * f_of_Y[1:]).sum(1) + c)\n",
    "    K_YY_mean = torch.mean(K_YY.pow(d))\n",
    "\n",
    "    K_XY = (alpha * (f_of_X[:-1] * f_of_Y[1:]).sum(1) + c)\n",
    "    K_XY_mean = torch.mean(K_XY.pow(d))\n",
    "\n",
    "    K_YX = (alpha * (f_of_Y[:-1] * f_of_X[1:]).sum(1) + c)\n",
    "    K_YX_mean = torch.mean(K_YX.pow(d))\n",
    "\n",
    "    return K_XX_mean + K_YY_mean - K_XY_mean - K_YX_mean\n",
    "\n",
    "\n",
    "def _mix_rbf_kernel(X, Y, sigma_list):\n",
    "    assert(X.size(0) == Y.size(0))\n",
    "    m = X.size(0)\n",
    "\n",
    "    Z = torch.cat((X, Y), 0)\n",
    "    ZZT = torch.mm(Z, Z.t())\n",
    "    diag_ZZT = torch.diag(ZZT).unsqueeze(1)\n",
    "    Z_norm_sqr = diag_ZZT.expand_as(ZZT)\n",
    "    exponent = Z_norm_sqr - 2 * ZZT + Z_norm_sqr.t()\n",
    "\n",
    "    K = 0.0\n",
    "    for sigma in sigma_list:\n",
    "        gamma = 1.0 / (2 * sigma**2)\n",
    "        K += torch.exp(-gamma * exponent)\n",
    "\n",
    "    return K[:m, :m], K[:m, m:], K[m:, m:], len(sigma_list)\n",
    "\n",
    "def _mix_imq_kernel(X,\n",
    "               Y,\n",
    "               sigma_list):\n",
    "\n",
    "    assert(X.size(0) == Y.size(0))\n",
    "    m = X.size(0)\n",
    "    h_dim = X.size(1)\n",
    "\n",
    "    Z = torch.cat((X, Y), 0)\n",
    "    ZZT = torch.mm(Z, Z.t())\n",
    "    diag_ZZT = torch.diag(ZZT).unsqueeze(1)\n",
    "    Z_norm_sqr = diag_ZZT.expand_as(ZZT)\n",
    "\n",
    "    exponent = Z_norm_sqr - 2 * ZZT + Z_norm_sqr.t()\n",
    "\n",
    "    K = 0.0\n",
    "    for sigma in sigma_list:\n",
    "        gamma = 2 * h_dim * 1.0 * sigma**2\n",
    "        K += gamma / (gamma + exponent)\n",
    "\n",
    "    return K[:m, :m], K[:m, m:], K[m:, m:], len(sigma_list)\n",
    "\n",
    "def mix_imq_mmd2(X, Y, sigma_list, biased=True):\n",
    "    K_XX, K_XY, K_YY, d = _mix_imq_kernel(X, Y, sigma_list)\n",
    "    # return _mmd2(K_XX, K_XY, K_YY, const_diagonal=d, biased=biased)\n",
    "    return _mmd2(K_XX, K_XY, K_YY, const_diagonal=False, biased=biased)\n",
    "\n",
    "def mix_rbf_mmd2(X, Y, sigma_list, biased=True):\n",
    "    K_XX, K_XY, K_YY, d = _mix_rbf_kernel(X, Y, sigma_list)\n",
    "    # return _mmd2(K_XX, K_XY, K_YY, const_diagonal=d, biased=biased)\n",
    "    return _mmd2(K_XX, K_XY, K_YY, const_diagonal=False, biased=biased)\n",
    "\n",
    "def mix_rbf_mmd2_loss(X, Y, sigma_list, biased=True):\n",
    "\n",
    "    mmd_dist_ref = mix_rbf_mmd2(X, Y, sigma_list, biased=True)\n",
    "    return torch.sqrt(F.relu(mmd_dist_ref))\n",
    "\n",
    "def mix_rbf_mmd2_unbiased_loss(X, Y, sigma_list):\n",
    "\n",
    "    K_XX, K_XY, K_YY, d = _mix_rbf_kernel(X, Y, sigma_list)\n",
    "    mmd_dist_ref = _mmd2_ignore_diagonals(K_XX, K_XY, K_YY, const_diagonal=False, biased=False)\n",
    "    return torch.sqrt(F.relu(torch.abs(mmd_dist_ref)))\n",
    "\n",
    "def mix_rbf_mmd2_and_ratio(X, Y, sigma_list, biased=True):\n",
    "    K_XX, K_XY, K_YY, d = _mix_rbf_kernel(X, Y, sigma_list)\n",
    "    # return _mmd2_and_ratio(K_XX, K_XY, K_YY, const_diagonal=d, biased=biased)\n",
    "    return _mmd2_and_ratio(K_XX, K_XY, K_YY, const_diagonal=False, biased=biased)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Helper functions to compute variances based on kernel matrices\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def _mmd2_ignore_diagonals(K_XX, K_XY, K_YY, const_diagonal=False, biased=False):\n",
    "    m = K_XX.size(0)    # assume X, Y are same shape\n",
    "\n",
    "    # Get the various sums of kernels that we'll use\n",
    "    # Kts drop the diagonal, but we don't need to compute them explicitly\n",
    "    if const_diagonal is not False:\n",
    "        diag_X = diag_Y = diag_XY = const_diagonal\n",
    "        sum_diag_X = sum_diag_Y = sum_diag_XY = m * const_diagonal\n",
    "    else:\n",
    "        diag_X = torch.diag(K_XX)            # (m,)\n",
    "        diag_Y = torch.diag(K_YY)            # (m,)\n",
    "        diag_XY = torch.diag(K_XY)           # (m,)\n",
    "        sum_diag_X = torch.sum(diag_X)\n",
    "        sum_diag_Y = torch.sum(diag_Y)\n",
    "        sum_diag_XY = torch.sum(diag_XY)\n",
    "\n",
    "\n",
    "    Kt_XX_sums = K_XX.sum(dim=1) - diag_X             # \\tilde{K}_XX * e = K_XX * e - diag_X\n",
    "    Kt_YY_sums = K_YY.sum(dim=1) - diag_Y             # \\tilde{K}_YY * e = K_YY * e - diag_Y\n",
    "    K_XY_sums_0 = K_XY.sum(dim=0) - diag_XY                     # K_{XY}^T * e\n",
    "\n",
    "    Kt_XX_sum = Kt_XX_sums.sum()                       # e^T * \\tilde{K}_XX * e\n",
    "    Kt_YY_sum = Kt_YY_sums.sum()                       # e^T * \\tilde{K}_YY * e\n",
    "    K_XY_sum = K_XY_sums_0.sum()                       # e^T * K_{XY} * e\n",
    "\n",
    "    if biased:\n",
    "        mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)\n",
    "            + (Kt_YY_sum + sum_diag_Y) / (m * m)\n",
    "            - 2.0 * (K_XY_sum + sum_diag_XY) / (m * m))\n",
    "    else:\n",
    "        mmd2 = ((Kt_XX_sum ) / (m * (m-1))\n",
    "            + (Kt_YY_sum) / (m * (m-1))\n",
    "            - 2.0 * (K_XY_sum) / (m * (m-1)))\n",
    "\n",
    "    return mmd2\n",
    "\n",
    "\n",
    "def _mmd2(K_XX, K_XY, K_YY, const_diagonal=False, biased=False):\n",
    "    m = K_XX.size(0)    # assume X, Y are same shape\n",
    "\n",
    "    # Get the various sums of kernels that we'll use\n",
    "    # Kts drop the diagonal, but we don't need to compute them explicitly\n",
    "    if const_diagonal is not False:\n",
    "        diag_X = diag_Y = const_diagonal\n",
    "        sum_diag_X = sum_diag_Y = m * const_diagonal\n",
    "    else:\n",
    "        diag_X = torch.diag(K_XX)                       # (m,)\n",
    "        diag_Y = torch.diag(K_YY)                       # (m,)\n",
    "        sum_diag_X = torch.sum(diag_X)\n",
    "        sum_diag_Y = torch.sum(diag_Y)\n",
    "\n",
    "    Kt_XX_sums = K_XX.sum(dim=1) - diag_X             # \\tilde{K}_XX * e = K_XX * e - diag_X\n",
    "    Kt_YY_sums = K_YY.sum(dim=1) - diag_Y             # \\tilde{K}_YY * e = K_YY * e - diag_Y\n",
    "    K_XY_sums_0 = K_XY.sum(dim=0)                     # K_{XY}^T * e\n",
    "\n",
    "    Kt_XX_sum = Kt_XX_sums.sum()                       # e^T * \\tilde{K}_XX * e\n",
    "    Kt_YY_sum = Kt_YY_sums.sum()                       # e^T * \\tilde{K}_YY * e\n",
    "    K_XY_sum = K_XY_sums_0.sum()                       # e^T * K_{XY} * e\n",
    "\n",
    "    if biased:\n",
    "        mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)\n",
    "            + (Kt_YY_sum + sum_diag_Y) / (m * m)\n",
    "            - 2.0 * K_XY_sum / (m * m))\n",
    "    else:\n",
    "        mmd2 = (Kt_XX_sum / (m * (m - 1))\n",
    "            + Kt_YY_sum / (m * (m - 1))\n",
    "            - 2.0 * K_XY_sum / (m * m))\n",
    "\n",
    "    return mmd2\n",
    "\n",
    "\n",
    "def _mmd2_and_ratio(K_XX, K_XY, K_YY, const_diagonal=False, biased=False):\n",
    "    mmd2, var_est = _mmd2_and_variance(K_XX, K_XY, K_YY, const_diagonal=const_diagonal, biased=biased)\n",
    "    loss = mmd2 / torch.sqrt(torch.clamp(var_est, min=min_var_est))\n",
    "    return loss, mmd2, var_est\n",
    "\n",
    "\n",
    "def _mmd2_and_variance(K_XX, K_XY, K_YY, const_diagonal=False, biased=False):\n",
    "    m = K_XX.size(0)    # assume X, Y are same shape\n",
    "\n",
    "    # Get the various sums of kernels that we'll use\n",
    "    # Kts drop the diagonal, but we don't need to compute them explicitly\n",
    "    if const_diagonal is not False:\n",
    "        diag_X = diag_Y = const_diagonal\n",
    "        sum_diag_X = sum_diag_Y = m * const_diagonal\n",
    "        sum_diag2_X = sum_diag2_Y = m * const_diagonal**2\n",
    "    else:\n",
    "        diag_X = torch.diag(K_XX)                       # (m,)\n",
    "        diag_Y = torch.diag(K_YY)                       # (m,)\n",
    "        sum_diag_X = torch.sum(diag_X)\n",
    "        sum_diag_Y = torch.sum(diag_Y)\n",
    "        sum_diag2_X = diag_X.dot(diag_X)\n",
    "        sum_diag2_Y = diag_Y.dot(diag_Y)\n",
    "\n",
    "    Kt_XX_sums = K_XX.sum(dim=1) - diag_X             # \\tilde{K}_XX * e = K_XX * e - diag_X\n",
    "    Kt_YY_sums = K_YY.sum(dim=1) - diag_Y             # \\tilde{K}_YY * e = K_YY * e - diag_Y\n",
    "    K_XY_sums_0 = K_XY.sum(dim=0)                     # K_{XY}^T * e\n",
    "    K_XY_sums_1 = K_XY.sum(dim=1)                     # K_{XY} * e\n",
    "\n",
    "    Kt_XX_sum = Kt_XX_sums.sum()                       # e^T * \\tilde{K}_XX * e\n",
    "    Kt_YY_sum = Kt_YY_sums.sum()                       # e^T * \\tilde{K}_YY * e\n",
    "    K_XY_sum = K_XY_sums_0.sum()                       # e^T * K_{XY} * e\n",
    "\n",
    "    Kt_XX_2_sum = (K_XX ** 2).sum() - sum_diag2_X      # \\| \\tilde{K}_XX \\|_F^2\n",
    "    Kt_YY_2_sum = (K_YY ** 2).sum() - sum_diag2_Y      # \\| \\tilde{K}_YY \\|_F^2\n",
    "    K_XY_2_sum  = (K_XY ** 2).sum()                    # \\| K_{XY} \\|_F^2\n",
    "\n",
    "    if biased:\n",
    "        mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)\n",
    "            + (Kt_YY_sum + sum_diag_Y) / (m * m)\n",
    "            - 2.0 * K_XY_sum / (m * m))\n",
    "    else:\n",
    "        mmd2 = (Kt_XX_sum / (m * (m - 1))\n",
    "            + Kt_YY_sum / (m * (m - 1))\n",
    "            - 2.0 * K_XY_sum / (m * m))\n",
    "\n",
    "    var_est = (\n",
    "        2.0 / (m**2 * (m - 1.0)**2) * (2 * Kt_XX_sums.dot(Kt_XX_sums) - Kt_XX_2_sum + 2 * Kt_YY_sums.dot(Kt_YY_sums) - Kt_YY_2_sum)\n",
    "        - (4.0*m - 6.0) / (m**3 * (m - 1.0)**3) * (Kt_XX_sum**2 + Kt_YY_sum**2)\n",
    "        + 4.0*(m - 2.0) / (m**3 * (m - 1.0)**2) * (K_XY_sums_1.dot(K_XY_sums_1) + K_XY_sums_0.dot(K_XY_sums_0))\n",
    "        - 4.0*(m - 3.0) / (m**3 * (m - 1.0)**2) * (K_XY_2_sum) - (8 * m - 12) / (m**5 * (m - 1)) * K_XY_sum**2\n",
    "        + 8.0 / (m**3 * (m - 1.0)) * (\n",
    "            1.0 / m * (Kt_XX_sum + Kt_YY_sum) * K_XY_sum\n",
    "            - Kt_XX_sums.dot(K_XY_sums_1)\n",
    "            - Kt_YY_sums.dot(K_XY_sums_0))\n",
    "        )\n",
    "    return mmd2, var_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15adb320-a51a-47a5-a706-e7a296b21818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code downloaded from https://github.com/msesia/deepknockoffs/tree/master/DeepKnockoffs\n",
    "\n",
    "def covariance_diff_biased(X, Xk, SigmaHat, Mask, scale=1.0):\n",
    "    \"\"\" Second-order loss function, as described in deep knockoffs manuscript\n",
    "    :param X: input data\n",
    "    :param Xk: generated knockoffs\n",
    "    :param SigmaHat: target covariance matrix\n",
    "    :param Mask: masking the diagonal of Cov(X,Xk)\n",
    "    :param scale: scaling the loss function\n",
    "    :return: second-order loss function\n",
    "    \"\"\"\n",
    "\n",
    "    # Center X,Xk\n",
    "    mX  = X  - torch.mean(X,0,keepdim=True)\n",
    "    mXk = Xk - torch.mean(Xk,0,keepdim=True)\n",
    "    # Compute covariance matrices\n",
    "    SXkXk = torch.mm(torch.t(mXk),mXk)/mXk.shape[0]\n",
    "    SXXk  = torch.mm(torch.t(mX),mXk)/mXk.shape[0]\n",
    "\n",
    "    # Compute loss\n",
    "    T  = (SigmaHat-SXkXk).pow(2).sum() / scale\n",
    "    T += (Mask*(SigmaHat-SXXk)).pow(2).sum() / scale\n",
    "    return T\n",
    "\n",
    "def create_checkpoint_name(pars):\n",
    "    \"\"\" Defines the filename of the network\n",
    "    :param pars: training hyper-parameters\n",
    "    :return: filename composed of the hyper-parameters\n",
    "    \"\"\"\n",
    "\n",
    "    checkpoint_name = 'net'\n",
    "    for key, value in pars.items():\n",
    "        checkpoint_name += '_' + key\n",
    "        if key == 'alphas':\n",
    "            for i in range(len(pars['alphas'])):\n",
    "                checkpoint_name += '_' + str(pars['alphas'][i])\n",
    "        else:\n",
    "            checkpoint_name += '_' + str(value)\n",
    "    return checkpoint_name\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    \"\"\" Saves the most updatated network to filename and store the previous\n",
    "    machine in filename + _prev.pth.tar' file\n",
    "    :param state: training state of the machine\n",
    "    :filename: filename to save the current machine\n",
    "    \"\"\"\n",
    "\n",
    "    # keep the previous model\n",
    "    if os.path.isfile(filename):\n",
    "        os.rename(filename, filename + '_prev.pth.tar')\n",
    "    # save new model\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def gen_batches(n_samples, batch_size, n_reps):\n",
    "    \"\"\" Divide input data into batches.\n",
    "    :param data: input data\n",
    "    :param batch_size: size of each batch\n",
    "    :return: data divided into batches\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    for rep_id in range(n_reps):\n",
    "        idx = np.random.permutation(n_samples)\n",
    "        for i in range(0, math.floor(n_samples/batch_size)*batch_size, batch_size):\n",
    "            window = np.arange(i,i+batch_size)\n",
    "            new_batch = idx[window]\n",
    "            batches += [new_batch]\n",
    "    return(batches)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\" Deep knockoff network\n",
    "    \"\"\"\n",
    "    def __init__(self, p, dim_h, family=\"continuous\"):\n",
    "        \"\"\" Constructor\n",
    "        :param p: dimensions of data\n",
    "        :param dim_h: width of the network (~6 layers are fixed)\n",
    "        :param family: data type, either \"continuous\" or \"binary\"\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.p = p\n",
    "        self.dim_h = dim_h\n",
    "        if (family==\"continuous\"):\n",
    "            self.main = nn.Sequential(\n",
    "                nn.Linear(2*self.p, self.dim_h, bias=False),\n",
    "                nn.BatchNorm1d(self.dim_h),\n",
    "                nn.PReLU(),\n",
    "                nn.Linear(self.dim_h, self.dim_h, bias=False),\n",
    "                nn.BatchNorm1d(self.dim_h),\n",
    "                nn.PReLU(),\n",
    "                nn.Linear(self.dim_h, self.dim_h, bias=False),\n",
    "                nn.BatchNorm1d(self.dim_h),\n",
    "                nn.PReLU(),\n",
    "                # nn.Linear(self.dim_h, self.dim_h, bias=False),\n",
    "                # nn.BatchNorm1d(self.dim_h),\n",
    "                # nn.PReLU(),\n",
    "                # nn.Linear(self.dim_h, self.dim_h, bias=False),\n",
    "                # nn.BatchNorm1d(self.dim_h),\n",
    "                # nn.PReLU(),\n",
    "                # nn.Linear(self.dim_h, self.dim_h, bias=False),\n",
    "                # nn.BatchNorm1d(self.dim_h),\n",
    "                # nn.PReLU(),\n",
    "                nn.Linear(self.dim_h, self.p),\n",
    "            )\n",
    "        elif (family==\"binary\"):\n",
    "            self.main = nn.Sequential(\n",
    "                nn.Linear(2*self.p, self.dim_h, bias=False),\n",
    "                nn.BatchNorm1d(self.dim_h, eps=1e-02),\n",
    "                nn.PReLU(),\n",
    "                nn.Linear(self.dim_h, self.dim_h, bias=False),\n",
    "                nn.BatchNorm1d(self.dim_h, eps=1e-02),\n",
    "                nn.PReLU(),\n",
    "                nn.Linear(self.dim_h, self.dim_h, bias=False),\n",
    "                nn.BatchNorm1d(self.dim_h, eps=1e-02),\n",
    "                nn.PReLU(),\n",
    "                nn.Linear(self.dim_h, self.dim_h, bias=False),\n",
    "                nn.BatchNorm1d(self.dim_h, eps=1e-02),\n",
    "                nn.PReLU(),\n",
    "                nn.Linear(self.dim_h, self.dim_h, bias=False),\n",
    "                nn.BatchNorm1d(self.dim_h, eps=1e-02),\n",
    "                nn.PReLU(),\n",
    "                nn.Linear(self.dim_h, self.dim_h, bias=False),\n",
    "                nn.BatchNorm1d(self.dim_h, eps=1e-02),\n",
    "                nn.PReLU(),\n",
    "                nn.Linear(self.dim_h, self.p),\n",
    "                nn.Sigmoid(),\n",
    "                nn.BatchNorm1d(self.p, eps=1e-02),\n",
    "            )\n",
    "        else:\n",
    "            sys.exit(\"Error: unknown family\");\n",
    "\n",
    "    def forward(self, x, noise):\n",
    "        \"\"\" Sample knockoff copies of the data\n",
    "        :param x: input data\n",
    "        :param noise: random noise seed\n",
    "        :returns the constructed knockoffs\n",
    "        \"\"\"\n",
    "        x_cat = torch.cat((x,noise),1)\n",
    "        x_cat[:,0::2] = x\n",
    "        x_cat[:,1::2] = noise\n",
    "        return self.main(x_cat)\n",
    "\n",
    "def norm(X, p=2):\n",
    "    if(p==np.inf):\n",
    "        return(torch.max(torch.abs(X)))\n",
    "    else:\n",
    "        return(torch.norm(X,p))\n",
    "\n",
    "class KnockoffMachine:\n",
    "    \"\"\" Deep Knockoff machine\n",
    "    \"\"\"\n",
    "    def __init__(self, pars, checkpoint_name=None, logs_name=None):\n",
    "        \"\"\" Constructor\n",
    "        :param pars: dictionary containing the following keys\n",
    "                'family': data type, either \"continuous\" or \"binary\"\n",
    "                'p': dimensions of data\n",
    "                'epochs': number of training epochs\n",
    "                'epoch_length': number of iterations over the full data per epoch\n",
    "                'batch_size': batch size\n",
    "                'test_size': size of test set\n",
    "                'lr': learning rate for main training loop\n",
    "                'lr_milestones': when to decrease learning rate, unused when equals to number of epochs\n",
    "                'dim_h': width of the network\n",
    "                'target_corr': target correlation between variables and knockoffs\n",
    "                'LAMBDA': penalty encouraging second-order knockoffs\n",
    "                'DELTA': decorrelation penalty hyper-parameter\n",
    "                'GAMMA': penalty for MMD distance\n",
    "                'alphas': kernel widths for the MMD measure (uniform weights)\n",
    "        :param checkpoint_name: location to save the machine\n",
    "        :param logs_name: location to save the logfile\n",
    "        \"\"\"\n",
    "        # architecture parameters\n",
    "        self.p = pars['p']\n",
    "        self.dim_h = pars['dim_h']\n",
    "        self.family = pars['family']\n",
    "\n",
    "        # optimization parameters\n",
    "        self.epochs = pars['epochs']\n",
    "        self.epoch_length = pars['epoch_length']\n",
    "        self.batch_size = pars['batch_size']\n",
    "        self.test_size = pars['test_size']\n",
    "        self.lr = pars['lr']\n",
    "        self.lr_milestones = pars['lr_milestones']\n",
    "\n",
    "        # loss function parameters\n",
    "        self.alphas = pars['alphas']\n",
    "        self.target_corr = torch.from_numpy(pars['target_corr']).float()\n",
    "        self.DELTA = pars['DELTA']\n",
    "        self.GAMMA = pars['GAMMA']\n",
    "        self.LAMBDA = pars['LAMBDA']\n",
    "\n",
    "        # noise seed\n",
    "        self.noise_std = 1.0\n",
    "        self.dim_noise = self.p\n",
    "\n",
    "        # higher-order discrepency function\n",
    "        self.matching_loss = mix_rbf_mmd2_loss\n",
    "        self.matching_param = self.alphas\n",
    "\n",
    "        # Normalize learning rate to avoid numerical issues\n",
    "        self.lr = self.lr / np.max([self.DELTA, self.GAMMA, self.GAMMA, self.LAMBDA, 1.0])\n",
    "\n",
    "        self.pars = pars\n",
    "        if checkpoint_name == None:\n",
    "            self.checkpoint_name = None\n",
    "            self.best_checkpoint_name = None\n",
    "        else:\n",
    "            self.checkpoint_name = checkpoint_name + \"_checkpoint.pth.tar\"\n",
    "            self.best_checkpoint_name = checkpoint_name + \"_best.pth.tar\"\n",
    "\n",
    "        if logs_name == None:\n",
    "            self.logs_name = None\n",
    "        else:\n",
    "            self.logs_name = logs_name\n",
    "\n",
    "        self.resume_epoch = 0\n",
    "\n",
    "        # init the network\n",
    "        self.net = Net(self.p, self.dim_h, family=self.family)\n",
    "\n",
    "    def compute_diagnostics(self, X, Xk, noise, test=False):\n",
    "        \"\"\" Evaluates the different components of the loss function\n",
    "        :param X: input data\n",
    "        :param Xk: knockoffs of X\n",
    "        :param noise: allocated tensor that is used to sample the noise seed\n",
    "        :param test: compute the components of the loss on train (False) or test (True)\n",
    "        :return diagnostics: a dictionary containing the following keys:\n",
    "                 'Mean' : distance between the means of X and Xk\n",
    "                 'Corr-Diag': correlation between X and Xk\n",
    "                 'Corr-Full: ||Cov(X,X) - Cov(Xk,Xk)||_F^2 / ||Cov(X,X)||_F^2\n",
    "                 'Corr-Swap: ||M(Cov(X,X) - Cov(Xk,Xk))||_F^2 / ||Cov(X,X)||_F^2\n",
    "                             where M is a mask that excludes the diagonal\n",
    "                 'Loss': the value of the loss function\n",
    "                 'MMD-Full': discrepancy between (X',Xk') and (Xk'',X'')\n",
    "                 'MMD-Swap': discrepancy between (X',Xk') and (X'',Xk'')_swap(s)\n",
    "        \"\"\"\n",
    "        # Initialize dictionary of diagnostics\n",
    "        diagnostics = dict()\n",
    "        if test:\n",
    "            diagnostics[\"Data\"] = \"test\"\n",
    "        else:\n",
    "            diagnostics[\"Data\"] = \"train\"\n",
    "\n",
    "        ##############################\n",
    "        # Second-order moments\n",
    "        ##############################\n",
    "\n",
    "        # Difference in means\n",
    "        D_mean = X.mean(0) - Xk.mean(0)\n",
    "        D_mean = (D_mean*D_mean).mean()\n",
    "        diagnostics[\"Mean\"] = D_mean.data.cpu().item()\n",
    "\n",
    "        # Center and scale X, Xk\n",
    "        mX = X - torch.mean(X,0,keepdim=True)\n",
    "        mXk = Xk - torch.mean(Xk,0,keepdim=True)\n",
    "        scaleX  = (mX*mX).mean(0,keepdim=True)\n",
    "        scaleXk = (mXk*mXk).mean(0,keepdim=True)\n",
    "\n",
    "        # Correlation between X and Xk\n",
    "        scaleX[scaleX==0] = 1.0   # Prevent division by 0\n",
    "        scaleXk[scaleXk==0] = 1.0 # Prevent division by 0\n",
    "        mXs  = mX  / torch.sqrt(scaleX)\n",
    "        mXks = mXk / torch.sqrt(scaleXk)\n",
    "        corr = (mXs*mXks).mean()\n",
    "        diagnostics[\"Corr-Diag\"] = corr.data.cpu().item()\n",
    "\n",
    "        # Cov(Xk,Xk)\n",
    "        Sigma = torch.mm(torch.t(mXs),mXs)/mXs.shape[0]\n",
    "        Sigma_ko = torch.mm(torch.t(mXks),mXks)/mXk.shape[0]\n",
    "        DK_2 = norm(Sigma_ko-Sigma) / norm(Sigma)\n",
    "        diagnostics[\"Corr-Full\"] = DK_2.data.cpu().item()\n",
    "\n",
    "        # Cov(Xk,X) excluding the diagonal elements\n",
    "        SigIntra_est = torch.mm(torch.t(mXks),mXs)/mXk.shape[0]\n",
    "        DS_2 = norm(self.Mask*(SigIntra_est-Sigma)) / norm(Sigma)\n",
    "        diagnostics[\"Corr-Swap\"] = DS_2.data.cpu().item()\n",
    "\n",
    "        ##############################\n",
    "        # Loss function\n",
    "        ##############################\n",
    "        _, loss_display, mmd_full, mmd_swap = self.loss(X[:noise.shape[0]], Xk[:noise.shape[0]], test=True)\n",
    "        diagnostics[\"Loss\"]  = loss_display.data.cpu().item()\n",
    "        diagnostics[\"MMD-Full\"] = mmd_full.data.cpu().item()\n",
    "        diagnostics[\"MMD-Swap\"] = mmd_swap.data.cpu().item()\n",
    "\n",
    "        # Return dictionary of diagnostics\n",
    "        return diagnostics\n",
    "\n",
    "    def loss(self, X, Xk, test=False):\n",
    "        \"\"\" Evaluates the loss function\n",
    "        :param X: input data\n",
    "        :param Xk: knockoffs of X\n",
    "        :param test: evaluate the MMD, regardless the value of GAMMA\n",
    "        :return loss: the value of the effective loss function\n",
    "                loss_display: a copy of the loss variable that will be used for display\n",
    "                mmd_full: discrepancy between (X',Xk') and (Xk'',X'')\n",
    "                mmd_swap: discrepancy between (X',Xk') and (X'',Xk'')_swap(s)\n",
    "        \"\"\"\n",
    "\n",
    "        # Divide the observations into two disjoint batches\n",
    "        n = int(X.shape[0]/2)\n",
    "        X1,Xk1 = X[:n], Xk[:n]\n",
    "        X2,Xk2 = X[n:(2*n)], Xk[n:(2*n)]\n",
    "\n",
    "        # Joint variables\n",
    "        Z1 = torch.cat((X1,Xk1),1)\n",
    "        Z2 = torch.cat((Xk2,X2),1)\n",
    "        Z3 = torch.cat((X2,Xk2),1).clone()\n",
    "        swap_inds = np.where(np.random.binomial(1,0.5,size=self.p))[0]\n",
    "        Z3[:,swap_inds] = Xk2[:,swap_inds]\n",
    "        Z3[:,swap_inds+self.p] = X2[:,swap_inds]\n",
    "\n",
    "        # Compute the discrepancy between (X,Xk) and (Xk,X)\n",
    "        mmd_full = 0.0\n",
    "        # Compute the discrepancy between (X,Xk) and (X,Xk)_s\n",
    "        mmd_swap = 0.0\n",
    "        if(self.GAMMA>0 or test):\n",
    "            # Evaluate the MMD by following section 4.3 in\n",
    "            # Li et al. \"Generative Moment Matching Networks\". Link to\n",
    "            # the manuscript -- https://arxiv.org/pdf/1502.02761.pdf\n",
    "            mmd_full = self.matching_loss(Z1, Z2, self.matching_param)\n",
    "            mmd_swap = self.matching_loss(Z1, Z3, self.matching_param)\n",
    "\n",
    "        # Match first two moments\n",
    "        loss_moments = 0.0\n",
    "        if self.LAMBDA>0:\n",
    "            # First moment\n",
    "            D_mean = X.mean(0) - Xk.mean(0)\n",
    "            loss_1m = D_mean.pow(2).sum()\n",
    "            # Second moments\n",
    "            loss_2m = covariance_diff_biased(X, Xk, self.SigmaHat, self.Mask, scale=self.Sigma_norm)\n",
    "            # Combine moments\n",
    "            loss_moments = loss_1m + loss_2m\n",
    "\n",
    "        # Penalize correlations between variables and knockoffs\n",
    "        loss_corr = 0.0\n",
    "        if self.DELTA>0:\n",
    "            # Center X and Xk\n",
    "            mX  = X  - torch.mean(X,0,keepdim=True)\n",
    "            mXk = Xk - torch.mean(Xk,0,keepdim=True)\n",
    "            # Correlation between X and Xk\n",
    "            eps = 1e-3\n",
    "            scaleX  = mX.pow(2).mean(0,keepdim=True)\n",
    "            scaleXk = mXk.pow(2).mean(0,keepdim=True)\n",
    "            mXs  = mX / (eps+torch.sqrt(scaleX))\n",
    "            mXks = mXk / (eps+torch.sqrt(scaleXk))\n",
    "            corr_XXk = (mXs*mXks).mean(0)\n",
    "            loss_corr = (corr_XXk-self.target_corr).pow(2).mean()\n",
    "\n",
    "        # Combine the loss functions\n",
    "        loss = self.GAMMA*mmd_full + self.GAMMA*mmd_swap + self.LAMBDA*loss_moments + self.DELTA*loss_corr\n",
    "        loss_display = loss\n",
    "        return loss, loss_display, mmd_full, mmd_swap\n",
    "\n",
    "\n",
    "    def train(self, X_in, resume = False):\n",
    "        \"\"\" Fit the machine to the training data\n",
    "        :param X_in: input data\n",
    "        :param resume: proceed the training by loading the last checkpoint\n",
    "        \"\"\"\n",
    "\n",
    "        # Divide data into training/test set\n",
    "        X = torch.from_numpy(X_in[self.test_size:]).float()\n",
    "        if(self.test_size>0):\n",
    "            X_test = torch.from_numpy(X_in[:self.test_size]).float()\n",
    "        else:\n",
    "            X_test = torch.zeros(0, self.p)\n",
    "\n",
    "        # used to compute statistics and diagnostics\n",
    "        self.SigmaHat = np.cov(X,rowvar=False)\n",
    "        self.SigmaHat = torch.from_numpy(self.SigmaHat).float()\n",
    "        self.Mask = torch.ones(self.p, self.p) - torch.eye(self.p)\n",
    "\n",
    "        # allocate a matrix for the noise realization\n",
    "        noise = torch.zeros(self.batch_size,self.dim_noise)\n",
    "        noise_test = torch.zeros(X_test.shape[0],self.dim_noise)\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "\n",
    "        if resume == True:  # load the last checkpoint\n",
    "            self.load(self.checkpoint_name)\n",
    "            self.net.train()\n",
    "        else:  # start learning from scratch\n",
    "            self.net.train()\n",
    "            # Define the optimization method\n",
    "            self.net_optim = optim.SGD(self.net.parameters(), lr = self.lr, momentum=0.9)\n",
    "            # Define the scheduler\n",
    "            self.net_sched = optim.lr_scheduler.MultiStepLR(self.net_optim, gamma=0.1,\n",
    "                                                            milestones=self.lr_milestones)\n",
    "\n",
    "        # bandwidth parameters of the Gaussian kernel\n",
    "        self.matching_param = self.alphas\n",
    "\n",
    "        # move data to GPU if available\n",
    "        if use_cuda:\n",
    "            self.SigmaHat = self.SigmaHat.cuda()\n",
    "            self.Mask = self.Mask.cuda()\n",
    "            self.net = self.net.cuda()\n",
    "            X = X.cuda()\n",
    "            X_test = X_test.cuda()\n",
    "            noise = noise.cuda()\n",
    "            noise_test = noise_test.cuda()\n",
    "            self.target_corr = self.target_corr.cuda()\n",
    "\n",
    "        Xk = 0*X\n",
    "        self.Sigma_norm = self.SigmaHat.pow(2).sum()\n",
    "        self.Sigma_norm_cross = (self.Mask*self.SigmaHat).pow(2).sum()\n",
    "\n",
    "        # Store diagnostics\n",
    "        diagnostics = pd.DataFrame()\n",
    "        losses_test = []\n",
    "\n",
    "        # main training loop\n",
    "        for epoch in range(self.resume_epoch, self.epochs):\n",
    "            # prepare for training phase\n",
    "            self.net.train()\n",
    "            # update the learning rate scheduler\n",
    "            self.net_sched.step()\n",
    "            # divide the data into batches\n",
    "            batches = gen_batches(X.size(0), self.batch_size, self.epoch_length)\n",
    "\n",
    "            losses = []\n",
    "            losses_dist_swap = []\n",
    "            losses_dist_full = []\n",
    "\n",
    "            for batch in batches:\n",
    "                # Extract data for this batch\n",
    "                X_batch  = X[batch,:]\n",
    "\n",
    "                self.net_optim.zero_grad()\n",
    "\n",
    "                # Run the network\n",
    "                Xk_batch = self.net(X_batch, self.noise_std*noise.normal_())\n",
    "\n",
    "                # Compute the loss function\n",
    "                loss, loss_display, mmd_full, mmd_swap = self.loss(X_batch, Xk_batch)\n",
    "\n",
    "                # Compute the gradient\n",
    "                loss.backward()\n",
    "\n",
    "                # Take a gradient step\n",
    "                self.net_optim.step()\n",
    "\n",
    "                # Save history\n",
    "                losses.append(loss_display.data.cpu().item())\n",
    "                if self.GAMMA>0:\n",
    "                    losses_dist_swap.append(mmd_swap.data.cpu().item())\n",
    "                    losses_dist_full.append(mmd_full.data.cpu().item())\n",
    "\n",
    "                # Save the knockoffs\n",
    "                Xk[batch, :] = Xk_batch.data\n",
    "\n",
    "            ##############################\n",
    "            # Compute diagnostics\n",
    "            ##############################\n",
    "\n",
    "            # Prepare for testing phase\n",
    "            self.net.eval()\n",
    "\n",
    "            # Evaluate the diagnostics on the training data, the following\n",
    "            # function recomputes the loss on the training data\n",
    "            diagnostics_train = self.compute_diagnostics(X, Xk, noise, test=False)\n",
    "            diagnostics_train[\"Loss\"] = np.mean(losses)\n",
    "            if(self.GAMMA>0 and self.GAMMA>0):\n",
    "                diagnostics_train[\"MMD-Full\"] = np.mean(losses_dist_full)\n",
    "                diagnostics_train[\"MMD-Swap\"] = np.mean(losses_dist_swap)\n",
    "            diagnostics_train[\"Epoch\"] = epoch\n",
    "            diagnostics = pd.concat([diagnostics, pd.DataFrame([diagnostics_train])], ignore_index=True)\n",
    "\n",
    "            # Evaluate the diagnostics on the test data if available\n",
    "            if(self.test_size>0):\n",
    "                Xk_test = self.net(X_test, self.noise_std*noise_test.normal_())\n",
    "                diagnostics_test = self.compute_diagnostics(X_test, Xk_test, noise_test, test=True)\n",
    "            else:\n",
    "                diagnostics_test = {key:np.nan for key in diagnostics_train.keys()}\n",
    "            diagnostics_test[\"Epoch\"] = epoch\n",
    "            diagnostics = pd.concat([diagnostics, pd.DataFrame([diagnostics_test])], ignore_index=True)\n",
    "\n",
    "            # If the test loss is at a minimum, save the machine to\n",
    "            # the location pointed by best_checkpoint_name\n",
    "            losses_test.append(diagnostics_test[\"Loss\"])\n",
    "            if((self.test_size>0) and (diagnostics_test[\"Loss\"] == np.min(losses_test)) and \\\n",
    "               (self.best_checkpoint_name is not None)):\n",
    "                best_machine = True\n",
    "                save_checkpoint({\n",
    "                    'epochs': epoch+1,\n",
    "                    'pars'  : self.pars,\n",
    "                    'state_dict': self.net.state_dict(),\n",
    "                    'optimizer' : self.net_optim.state_dict(),\n",
    "                    'scheduler' : self.net_sched.state_dict(),\n",
    "                }, self.best_checkpoint_name)\n",
    "            else:\n",
    "                best_machine = False\n",
    "\n",
    "            ##############################\n",
    "            # Print progress\n",
    "            ##############################\n",
    "            if(self.test_size>0):\n",
    "                print(\"[%4d/%4d], Loss: (%.4f, %.4f)\" %\n",
    "                      (epoch + 1, self.epochs, diagnostics_train[\"Loss\"], diagnostics_test[\"Loss\"]), end=\", \")\n",
    "                print(\"MMD: (%.4f,%.4f)\" %\n",
    "                      (diagnostics_train[\"MMD-Full\"]+diagnostics_train[\"MMD-Swap\"], \n",
    "                       diagnostics_test[\"MMD-Full\"]+diagnostics_test[\"MMD-Swap\"]), end=\", \")\n",
    "                print(\"Cov: (%.3f,%.3f)\" %\n",
    "                      (diagnostics_train[\"Corr-Full\"]+diagnostics_train[\"Corr-Swap\"], \n",
    "                       diagnostics_test[\"Corr-Full\"]+diagnostics_test[\"Corr-Swap\"]), end=\", \")\n",
    "                print(\"Decorr: (%.3f,%.3f)\" %\n",
    "                      (diagnostics_train[\"Corr-Diag\"], diagnostics_test[\"Corr-Diag\"]), end=\"\")\n",
    "                if best_machine:\n",
    "                    print(\" *\", end=\"\")\n",
    "            else:\n",
    "                print(\"[%4d/%4d], Loss: %.4f\" %\n",
    "                      (epoch + 1, self.epochs, diagnostics_train[\"Loss\"]), end=\", \")\n",
    "                print(\"MMD: %.4f\" %\n",
    "                      (diagnostics_train[\"MMD-Full\"] + diagnostics_train[\"MMD-Swap\"]), end=\", \")\n",
    "                print(\"Cov: %.3f\" %\n",
    "                      (diagnostics_train[\"Corr-Full\"] + diagnostics_train[\"Corr-Swap\"]), end=\", \")\n",
    "                print(\"Decorr: %.3f\" %\n",
    "                      (diagnostics_train[\"Corr-Diag\"]), end=\"\")\n",
    "                \n",
    "            print(\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Save diagnostics to logfile\n",
    "            if self.logs_name is not None:\n",
    "                diagnostics.to_csv(self.logs_name, sep=\" \", index=False)\n",
    "\n",
    "            # Save the current machine to location checkpoint_name\n",
    "            if self.checkpoint_name is not None:\n",
    "                save_checkpoint({\n",
    "                    'epochs': epoch+1,\n",
    "                    'pars'  : self.pars,\n",
    "                    'state_dict': self.net.state_dict(),\n",
    "                    'optimizer' : self.net_optim.state_dict(),\n",
    "                    'scheduler' : self.net_sched.state_dict(),\n",
    "                }, self.checkpoint_name)\n",
    "\n",
    "    def load(self, checkpoint_name):\n",
    "        \"\"\" Load a machine from a stored checkpoint\n",
    "        :param checkpoint_name: checkpoint name of a trained machine\n",
    "        \"\"\"\n",
    "        filename = checkpoint_name + \"_checkpoint.pth.tar\"\n",
    "\n",
    "        flag = 1\n",
    "        if os.path.isfile(filename):\n",
    "            print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "            sys.stdout.flush()\n",
    "            try:\n",
    "                checkpoint = torch.load(filename, map_location='cpu')\n",
    "            except:\n",
    "                print(\"error loading saved model, trying the previous version\")\n",
    "                sys.stdout.flush()\n",
    "                flag = 0\n",
    "\n",
    "            if flag == 0:\n",
    "                try:\n",
    "                    checkpoint = torch.load(filename + '_prev.pth.tar', map_location='cpu')\n",
    "                    flag = 1\n",
    "                except:\n",
    "                    print(\"error loading prev model, starting from scratch\")\n",
    "                    sys.stdout.flush()\n",
    "                    flag = 0\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "            sys.stdout.flush()\n",
    "            flag = 0\n",
    "\n",
    "        if flag == 1:\n",
    "                self.net.load_state_dict(checkpoint['state_dict'])\n",
    "                if torch.cuda.is_available():\n",
    "                    self.net = self.net.cuda()\n",
    "\n",
    "                self.net_optim = optim.SGD(self.net.parameters(), lr = self.lr, momentum=0.9)\n",
    "                self.net_optim.load_state_dict(checkpoint['optimizer'])\n",
    "                self.net_sched = optim.lr_scheduler.MultiStepLR(self.net_optim, gamma=0.1,\n",
    "                                                                milestones=self.lr_milestones)\n",
    "                self.resume_epoch = checkpoint['epochs']\n",
    "\n",
    "                print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                      .format(filename, checkpoint['epochs']))\n",
    "                sys.stdout.flush()\n",
    "        else:\n",
    "            self.net.train()\n",
    "            self.net_optim = optim.SGD(self.net.parameters(), lr = self.lr, momentum=0.9)\n",
    "            self.net_sched = optim.lr_scheduler.MultiStepLR(self.net_optim, gamma=0.1,\n",
    "                                                            milestones=self.lr_milestones)\n",
    "\n",
    "            self.resume_epoch = 0\n",
    "\n",
    "    def generate(self, X_in):\n",
    "        \"\"\" Generate knockoff copies\n",
    "        :param X_in: data samples\n",
    "        :return Xk: knockoff copy per each sample in X\n",
    "        \"\"\"\n",
    "\n",
    "        X = torch.from_numpy(X_in).float()\n",
    "        self.net = self.net.cpu()\n",
    "        self.net.eval()\n",
    "\n",
    "        # Run the network in evaluation mode\n",
    "        Xk = self.net(X, self.noise_std*torch.randn(X.size(0),self.dim_noise))\n",
    "        Xk = Xk.data.cpu().numpy()\n",
    "\n",
    "        return Xk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9484c458-35ef-44ca-a013-cd2f91b113fa",
   "metadata": {},
   "source": [
    "### KNN Knockoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afc15a8b-94a1-4fa2-b9e4-df86de6b33cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_knockoffs_knn(X: pd.DataFrame,\n",
    "                            types: List[str],\n",
    "                            topk_features: int = 10,\n",
    "                            nbrs_per_sample: int = 5,\n",
    "                            add_noise_scale: float = 1e-6,\n",
    "                            random_state: int = 0) -> pd.DataFrame:\n",
    "    np.random.seed(random_state)\n",
    "    X_np = X.values.copy()\n",
    "    n, p = X_np.shape\n",
    "    X_tilde = np.zeros_like(X_np)\n",
    "\n",
    "    # compute Pearson correlation (handle constant columns)\n",
    "    corr = np.corrcoef(np.nan_to_num(X_np.T))\n",
    "    corr = np.nan_to_num(corr)  # replace nan\n",
    "\n",
    "    for j in range(p):\n",
    "        col_j = X_np[:, j]\n",
    "        # get absolute correlations with other features\n",
    "        abs_corrs = np.abs(corr[j, :])\n",
    "        # exclude self\n",
    "        abs_corrs[j] = -np.inf\n",
    "        # indices of top-k correlated features\n",
    "        topk_idx = np.argsort(abs_corrs)[::-1][:topk_features]\n",
    "        # weights for distances: proportional to correlation strength\n",
    "        w = abs_corrs[topk_idx].astype(float)\n",
    "        if np.sum(w) <= 0:\n",
    "            # fallback: equal weights\n",
    "            w = np.ones_like(w)\n",
    "        w = w / np.max(w)  # normalize so max is 1\n",
    "\n",
    "        # build the feature-subspace matrix for nearest neighbor distances\n",
    "        # If topk_features == 0 (no other features), use only j itself (this degenerates)\n",
    "        if len(topk_idx) == 0:\n",
    "            # degenerate: use the feature itself (will make samples identical)\n",
    "            subspace = col_j.reshape(-1, 1)\n",
    "            weights = np.array([1.0])\n",
    "        else:\n",
    "            subspace = X_np[:, topk_idx]  # shape (n, k)\n",
    "            weights = w  # shape (k,)\n",
    "\n",
    "        # compute pairwise distances with weights: d_ij^2 = sum_k w_k * (x_ik - x_jk)^2\n",
    "        # scale columns by sqrt(weights) then use Euclidean distance.\n",
    "        sqrt_w = np.sqrt(weights)\n",
    "        # handle columns alignment: if subspace is 1D, make it 2D\n",
    "        if subspace.ndim == 1:\n",
    "            subspace = subspace.reshape(-1, 1)\n",
    "        scaled_subspace = subspace * sqrt_w \n",
    "\n",
    "        nbrs = NearestNeighbors(n_neighbors=min(n, nbrs_per_sample + 1), algorithm='auto').fit(scaled_subspace)\n",
    "        distances, indices = nbrs.kneighbors(scaled_subspace, return_distance=True)  # includes self at pos 0\n",
    "\n",
    "        # For each sample i, pick neighbor indices (excluding itself) and sample knockoff value\n",
    "        for i in range(n):\n",
    "            neighs = indices[i, :]\n",
    "            # exclude self if present (distance zero)\n",
    "            if neighs[0] == i:\n",
    "                neighs = neighs[1: nbrs_per_sample + 1]\n",
    "            else:\n",
    "                neighs = neighs[:nbrs_per_sample]\n",
    "            neighs = neighs[neighs != i]\n",
    "            if len(neighs) == 0:\n",
    "                if types[j] == \"cont\":\n",
    "                    std_j = np.nanstd(col_j) if np.nanstd(col_j) > 0 else 1.0\n",
    "                    X_tilde[i, j] = col_j[i] + np.random.normal(scale=add_noise_scale * std_j)\n",
    "                else:  # binary\n",
    "                    X_tilde[i, j] = col_j[i]\n",
    "                continue\n",
    "\n",
    "            neigh_vals = col_j[neighs]\n",
    "            # if target is binary -> sample Bernoulli from neighbor frequency\n",
    "            if types[j] == \"bin\" or is_binary_col(col_j):\n",
    "                # neighbors may have NaNs: drop\n",
    "                neigh_vals_clean = neigh_vals[~np.isnan(neigh_vals)]\n",
    "                if len(neigh_vals_clean) == 0:\n",
    "                    p_hat = 0.5\n",
    "                else:\n",
    "                    # p_hat = weighted mean by distance (closer neighbors contribute more)\n",
    "                    # weights by inverse distance (avoid div by zero)\n",
    "                    d = distances[i, 1: 1 + len(neigh_vals_clean)]\n",
    "                    d = np.maximum(d, 1e-8)\n",
    "                    invd = 1 / d\n",
    "                    p_hat = np.average(neigh_vals_clean, weights=invd)\n",
    "                X_tilde[i, j] = np.random.binomial(1, p_hat)\n",
    "            else:\n",
    "                # continuous: sample a neighbor's value with probability proportional to 1/distance\n",
    "                d = distances[i, :len(neigh_vals)]\n",
    "                d = np.maximum(d, 1e-8)  # avoid zero\n",
    "                probs = 1.0 / d\n",
    "                probs = probs / probs.sum()\n",
    "                chosen_idx = np.random.choice(len(neigh_vals), p=probs)\n",
    "                chosen_val = neigh_vals[chosen_idx]\n",
    "                # optionally add small Gaussian jitter proportional to std of j\n",
    "                std_j = np.nanstd(col_j) if np.nanstd(col_j) > 0 else 1.0\n",
    "                X_tilde[i, j] = chosen_val + np.random.normal(scale=add_noise_scale * std_j)\n",
    "\n",
    "    X_tilde_df = pd.DataFrame(X_tilde, columns=[f\"{c}_knock\" for c in X.columns], index=X.index)\n",
    "    return X_tilde_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf85e9a4-b69f-4d1f-a2df-ea84a2c3efd2",
   "metadata": {},
   "source": [
    "### Gaussian Knockoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a7d4c9e-5f4b-4a80-995e-ee5f4e5bb5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_knockoff(X):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    cov = np.cov(X, rowvar=False)\n",
    "    X_tilde = np.random.multivariate_normal(mean, cov, size=X.shape[0])\n",
    "    return X_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820f5bb4-00b6-4f3a-8b11-5b88f7f85e49",
   "metadata": {},
   "source": [
    "### Defining Binary vs. Continuous Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fae75b6-f6ae-4715-ba12-7fd1d48b6811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_binary_col(col: np.ndarray, tol=1e-8):\n",
    "    uniques = np.unique(col[~np.isnan(col)])\n",
    "    return set(uniques).issubset({0, 1}) and len(uniques) <= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d7f45-b516-48e4-b5a2-e3d433fded39",
   "metadata": {},
   "source": [
    "## Permutation Functions for Obtaining P Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8378254d-edb3-404a-bc35-f9fa6b786d2b",
   "metadata": {},
   "source": [
    "### Method with Knockoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8989d819-d67b-4efe-a315-a84f1ae77b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_pvalues(\n",
    "    X,\n",
    "    X_knockoffs,\n",
    "    y,\n",
    "    task=\"classification\",\n",
    "    ntree=500,\n",
    "    mtry=None,\n",
    "    B=50,\n",
    "    random_state=123,\n",
    "):\n",
    "    np.random.seed(random_state)\n",
    "    X = pd.DataFrame(X).reset_index(drop=True)\n",
    "    X_knockoffs = pd.DataFrame(X_knockoffs).reset_index(drop=True)\n",
    "\n",
    "    n, p = X.shape\n",
    "\n",
    "    # Combine X and knockoffs\n",
    "    combined_data = pd.concat([X, X_knockoffs], axis=1)\n",
    "    combined_data.columns = (\n",
    "        [f\"{col}_orig\" for col in X.columns]\n",
    "        + [f\"{col}_knockoff\" for col in X.columns]\n",
    "    )\n",
    "\n",
    "    # Choose model type\n",
    "    Model = RandomForestClassifier if task == \"classification\" else RandomForestRegressor\n",
    "    model_kwargs = dict(\n",
    "        n_estimators=ntree,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        max_features=mtry if mtry is not None else \"sqrt\",\n",
    "    )\n",
    "\n",
    "    rf = Model(**model_kwargs)\n",
    "    rf.fit(combined_data, y)\n",
    "    imp_scores = rf.feature_importances_\n",
    "\n",
    "    orig_imp = imp_scores[:p]\n",
    "    knock_imp = imp_scores[p:]\n",
    "    W_obs = orig_imp - knock_imp\n",
    "\n",
    "    W_perm = np.zeros((p, B))\n",
    "\n",
    "    for b in range(B):\n",
    "        y_perm = np.random.permutation(y)\n",
    "        rf_perm = Model(**model_kwargs)\n",
    "        rf_perm.fit(combined_data, y_perm)\n",
    "        imp_perm = rf_perm.feature_importances_\n",
    "\n",
    "        W_perm[:, b] = imp_perm[:p] - imp_perm[p:]\n",
    "\n",
    "        if (b + 1) % max(1, B // 10) == 0:\n",
    "            print(f\"Permutation {b + 1}/{B} done\")\n",
    "\n",
    "    pvals = np.zeros(p)\n",
    "    for j in range(p):\n",
    "        w_obs = W_obs[j]\n",
    "        w_null = W_perm[j, :]\n",
    "        if w_obs >= 0:\n",
    "            pvals[j] = (np.sum(w_null >= w_obs) + 1) / (B + 1)\n",
    "        else:\n",
    "            pvals[j] = (np.sum(w_null <= w_obs) + 1) / (B + 1)\n",
    "\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"variable\": X.columns,\n",
    "        \"importance_diff\": W_obs,\n",
    "        \"p_value\": pvals\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce93b196-2da1-471d-986c-294402b103ba",
   "metadata": {},
   "source": [
    "### Method without Knockoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e16ef636-5cce-45ae-aaa4-e5849f1f8e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_mda_pvalues(\n",
    "    X,\n",
    "    y,\n",
    "    task=\"classification\",\n",
    "    ntree=500,\n",
    "    mtry=None,\n",
    "    B=50,\n",
    "    random_state=123\n",
    "):\n",
    "    np.random.seed(random_state)\n",
    "    X = pd.DataFrame(X).reset_index(drop=True)\n",
    "    n, p = X.shape\n",
    "\n",
    "    Model = RandomForestClassifier if task==\"classification\" else RandomForestRegressor\n",
    "    model_kwargs = dict(\n",
    "        n_estimators=ntree,\n",
    "        max_features=mtry if mtry is not None else \"sqrt\",\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    rf = Model(**model_kwargs)\n",
    "    rf.fit(X, y)\n",
    "\n",
    "    if task == \"classification\":\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        baseline_score = accuracy_score(y, rf.predict(X))\n",
    "        score_fn = accuracy_score\n",
    "    else:\n",
    "        from sklearn.metrics import r2_score\n",
    "        baseline_score = r2_score(y, rf.predict(X))\n",
    "        score_fn = r2_score\n",
    "\n",
    "    def mda_feature(j, Xdata, y_true, baseline):\n",
    "        X_permuted = Xdata.copy()\n",
    "        X_permuted.iloc[:, j] = shuffle(X_permuted.iloc[:, j], random_state=random_state)\n",
    "        score = score_fn(y_true, rf.predict(X_permuted))\n",
    "        return baseline - score\n",
    "\n",
    "    # observed MDA importances\n",
    "    obs_imp = np.array([mda_feature(j, X, y, baseline_score) for j in range(p)])\n",
    "\n",
    "    # permutation nulls\n",
    "    null_imp = np.zeros((p, B))\n",
    "    for b in range(B):\n",
    "        y_perm = np.random.permutation(y)\n",
    "        rf.fit(X, y_perm)\n",
    "        baseline_perm = score_fn(y_perm, rf.predict(X))\n",
    "        null_imp[:, b] = np.array([mda_feature(j, X, y_perm, baseline_perm) for j in range(p)])\n",
    "        if (b+1) % 5 == 0 or b == 0:\n",
    "            print(f\"Permutation {b+1}/{B} done\")\n",
    "\n",
    "    # empirical p-values\n",
    "\n",
    "    pvals = np.zeros(p)\n",
    "\n",
    "    for j in range(p):\n",
    "        if obs_imp[j] >= 0:\n",
    "            pvals[j] = (np.sum(null_imp[j, :] >= obs_imp[j]) + 1) / (B + 1)\n",
    "        else:\n",
    "            pvals[j] = (np.sum(null_imp[j, :] <= obs_imp[j]) + 1) / (B + 1)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"variable\": X.columns,\n",
    "        \"mda_importance\": obs_imp,\n",
    "        \"p_value\": pvals\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3204b92-ba57-451e-b4f5-da38135824f7",
   "metadata": {},
   "source": [
    "## Simulation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c1d956-0c88-40be-8a26-a015625f68b0",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f406038-23b8-4162-9331-6fbbea6e8168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit  # sigmoid\n",
    "\n",
    "np.random.seed(47965636)\n",
    "n = 1000  # number of samples\n",
    "\n",
    "# --- 3 non-normal variables ---\n",
    "X1 = np.random.uniform(-2, 2, n)       # uniform\n",
    "X2 = np.random.exponential(1, n)       # exponential\n",
    "X3 = np.random.binomial(1, 0.4, n)     # binary\n",
    "\n",
    "# --- 5 normal variables ---\n",
    "X4 = np.random.normal(0, 1, n)\n",
    "X5 = np.random.normal(0, 1, n)\n",
    "X6 = np.clip(np.random.poisson(4, n), 0, 5)\n",
    "X6 = (X6 - np.mean(X6)) / np.std(X6)\n",
    "X7 = np.random.normal(0, 1, n)\n",
    "X8 = np.random.normal(0, 1, n)\n",
    "\n",
    "# --- heavy-tailed variables ---\n",
    "X9  = np.random.gamma(shape=2.0, scale=1.5, size=n)      # right-skewed (Gamma)\n",
    "X10 = np.random.lognormal(mean=0.0, sigma=1.0, size=n)   # strong right skew (Log-normal)\n",
    "X11 = np.random.pareto(a=2.5, size=n) + 1                # heavy tail (Pareto)\n",
    "\n",
    "# --- Combine into a matrix ---\n",
    "X = np.column_stack([X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11])\n",
    "\n",
    "# --- Define base effects (linear ones) ---\n",
    "beta = np.array([0.8, 0.8, 0.0, 0.8, 0.8, 0.8, 0.0, 0.0, 0.8, 0.8, 0.0]) # 1,2,4,5,6,9,10 all have effects\n",
    "#beta = np.array([0.3, 0.3, 0.0, 0.3, 0.3, 0.3, 0.0, 0.0, 0.3, 0.3, 0.0]) # 1,2,4,5,6,9,10 all have effects\n",
    "\n",
    "# --- Add nonlinear transformations ---\n",
    "# X1 has sinusoidal effect\n",
    "# X9 has log effect (heavy-tailed variable with diminishing effect)\n",
    "lin_pred = (\n",
    "    1.5 * np.sin(np.pi * X1 / 2) +           # nonlinear in X1\n",
    "    0.8 * np.log1p(X9) +                     # nonlinear in X9\n",
    "    X @ beta                                 # rest linear\n",
    ")\n",
    "\n",
    "# --- Generate binary outcome via logistic link ---\n",
    "p = expit(lin_pred)\n",
    "Y = np.random.binomial(1, p)\n",
    "\n",
    "# --- Put into DataFrame ---\n",
    "cols = [f\"X{i+1}\" for i in range(11)]\n",
    "df = pd.DataFrame(X, columns=cols)\n",
    "df[\"Y\"] = Y\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# --- Determine variable types automatically ---\n",
    "def is_binary_col(col):\n",
    "    return np.isin(np.unique(col), [0, 1]).all()\n",
    "\n",
    "types = [\"bin\" if is_binary_col(df[c].values) else \"cont\" for c in cols]\n",
    "\n",
    "# Expected logistic slope\n",
    "avg_slope = np.mean(p * (1 - p))\n",
    "\n",
    "# Compute variance contribution of each term\n",
    "var_contrib = np.zeros(X.shape[1])\n",
    "\n",
    "for j in range(X.shape[1]):\n",
    "    if j == 0:\n",
    "        term = 1.5 * np.sin(np.pi * X1 / 2)\n",
    "    elif j == 8:\n",
    "        term = 0.8 * np.log1p(X9)\n",
    "    else:\n",
    "        term = X[:, j] * beta[j]\n",
    "    var_contrib[j] = np.var(term) * avg_slope\n",
    "\n",
    "df_contrib = pd.DataFrame({\n",
    "    \"variable\": [f\"X{i+1}\" for i in range(11)],\n",
    "    \"var_contrib\": var_contrib\n",
    "})\n",
    "\n",
    "df_contrib = df_contrib.sort_values(\"var_contrib\", ascending=False).reset_index(drop=True)\n",
    "print(df_contrib)\n",
    "X = pd.DataFrame(X, columns=cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e129a-315b-490e-8bcb-1ec57a267b29",
   "metadata": {},
   "source": [
    "### KNN Knockoff P Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae4c301-11a1-4c0e-95d4-a2a084e2397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "knockoff_knn = construct_knockoffs_knn(X, types,nbrs_per_sample=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0016ea6-2810-47e8-b9a8-ab3e436097ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_knn = permutation_pvalues(X, knockoff_knn, Y, B=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e1447-0f49-4d0b-92bd-20d82693bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reject, pvals_corrected, _, _ = multipletests(perm_knn['p_value'], method='fdr_bh')\n",
    "\n",
    "# add the adjusted p-values to your DF\n",
    "perm_knn['p_value_bh'] = pvals_corrected\n",
    "\n",
    "# sort by the adjusted p-values\n",
    "perm_knn_sorted = perm_knn.sort_values('p_value_bh').reset_index(drop=True)\n",
    "\n",
    "perm_knn_sorted.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a736cc-d182-4136-8d84-32bc247d4167",
   "metadata": {},
   "source": [
    "### Gaussian Knockoff P Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00d30e-49e1-426f-a389-0efed61e8a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "knockoff_gauss = gaussian_knockoff(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00af9f2d-d117-4e8b-9c0e-38538b2479b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_gauss = permutation_pvalues(X, knockoff_gauss, Y, B=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f1e4b5-2a2d-4df2-a7e0-d0adae8757c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reject, pvals_corrected, _, _ = multipletests(perm_gauss['p_value'], method='fdr_bh')\n",
    "\n",
    "# add the adjusted p-values to DF\n",
    "perm_gauss['p_value_bh'] = pvals_corrected\n",
    "\n",
    "# sort by the adjusted p-values\n",
    "perm_gauss_sorted = perm_gauss.sort_values('p_value_bh').reset_index(drop=True)\n",
    "\n",
    "perm_gauss_sorted.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7a9bb0-a415-4f9c-bbd2-df38def8a628",
   "metadata": {},
   "source": [
    "### Deep Knockoff P Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5003d-18f8-42e0-b78d-b1e62253e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = X.shape[1]\n",
    "qt = QuantileTransformer(output_distribution='normal')\n",
    "X_scaled = qt.fit_transform(X)\n",
    "X_scaled = np.array(X_scaled)\n",
    "SigmaHat = np.cov(X_scaled, rowvar=False)\n",
    "second_order = GaussianKnockoffs(SigmaHat, mu=np.mean(X_scaled,0), method=\"sdp\")\n",
    "corr_g = (np.diag(SigmaHat) - np.diag(second_order.Ds)) / np.diag(SigmaHat)\n",
    "\n",
    "# Set the parameters for training deep knockoffs\n",
    "pars = dict()\n",
    "# Number of epochs\n",
    "pars['epochs'] = 100\n",
    "# Number of iterations over the full data per epoch\n",
    "pars['epoch_length'] = 50\n",
    "# Data type, either \"continuous\" or \"binary\"\n",
    "pars['family'] = \"continuous\"\n",
    "# Dimensions of the data\n",
    "pars['p'] = p\n",
    "# Size of the test set\n",
    "pars['test_size']  = int(0.1*n)\n",
    "# Batch size\n",
    "pars['batch_size'] = int(0.45*n)\n",
    "# Learning rate\n",
    "pars['lr'] = 0.003\n",
    "# When to decrease learning rate (unused when equal to number of epochs)\n",
    "pars['lr_milestones'] = [pars['epochs']]\n",
    "# Width of the network (number of layers is fixed to 6)\n",
    "pars['dim_h'] = int(10*p)\n",
    "# Penalty for the MMD distance\n",
    "pars['GAMMA'] = 0.5\n",
    "# Penalty encouraging second-order knockoffs\n",
    "pars['LAMBDA'] = 0.5\n",
    "# Decorrelation penalty hyperparameter\n",
    "pars['DELTA'] = 0.5\n",
    "# Target pairwise correlations between variables and knockoffs\n",
    "pars['target_corr'] = corr_g\n",
    "# Kernel widths for the MMD measure (uniform weights)\n",
    "pars['alphas'] = [1.,2.,4.,8.,16.,32.,64.,128.]\n",
    "\n",
    "# Initialize the machine\n",
    "machine = KnockoffMachine(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6eccac-c315-4bbd-8b68-fda2c176fa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fitting the knockoff machine...\")\n",
    "machine.train(X_scaled)\n",
    "X_deepknockoff = machine.generate(X_scaled)\n",
    "print(\"Size of the deep knockoff dataset: %d x %d.\" %(X_deepknockoff.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cdc398-f7ee-43b4-8fab-1f5f5b88154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_deep = permutation_pvalues(X_scaled, X_deepknockoff, Y, B=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa578d73-ed8a-4007-a17e-ca5abbd0fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "reject, pvals_corrected, _, _ = multipletests(perm_deep['p_value'], method='fdr_bh')\n",
    "\n",
    "# add the adjusted p-values to DF\n",
    "perm_deep['p_value_bh'] = pvals_corrected\n",
    "\n",
    "# sort by the adjusted p-values\n",
    "perm_deep_sorted = perm_deep.sort_values('p_value_bh').reset_index(drop=True)\n",
    "\n",
    "perm_deep_sorted.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9d2466-03b8-4634-8f72-be922da0964a",
   "metadata": {},
   "source": [
    "### Permutation Only - No Knockoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67004cb8-5a78-4cec-88b7-cf5f5e005b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_only = permutation_mda_pvalues(X,Y, B=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea9f719-2424-43c8-aa1f-e10417d9f793",
   "metadata": {},
   "outputs": [],
   "source": [
    "reject, pvals_corrected, _, _ = multipletests(perm_only['p_value'], method='fdr_bh')\n",
    "\n",
    "# add the adjusted p-values to DF\n",
    "perm_only['p_value_bh'] = pvals_corrected\n",
    "\n",
    "# sort by the adjusted p-values\n",
    "perm_only_sorted = perm_only.sort_values('p_value_bh').reset_index(drop=True)\n",
    "\n",
    "perm_only_sorted.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15901cb-7087-4785-a33d-e0c88fa44f49",
   "metadata": {},
   "source": [
    "## Simulation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd04139-3580-4aca-88cf-485530aedc73",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d9d51-6bd2-4f01-9b11-39145871ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import poisson\n",
    "\n",
    "\n",
    "np.random.seed(47965636)\n",
    "n = 1000\n",
    "p = 11 \n",
    "\n",
    "# --- correlation strengths ---\n",
    "rho_block_strong = 0.3   # within strong block (X4X6 + X9X10)\n",
    "rho_pair = 0.15          # X1,X2 pair\n",
    "rho_cross = 0.05         # weak across blocks\n",
    "\n",
    "# initialize covariance matrix\n",
    "cov = np.full((p, p), rho_cross)\n",
    "np.fill_diagonal(cov, 1.0)\n",
    "\n",
    "# define correlated groups\n",
    "pair_idx = [0, 1]                # X1, X2\n",
    "strong_block_idx = [3, 4, 5, 8, 9]  # X4,X5,X6,X9,X10 (strong block)\n",
    "independent_idx = [2, 6, 7, 10]     # weakly correlated (X3,X7,X8,X11)\n",
    "\n",
    "# apply pair correlation\n",
    "for i in pair_idx:\n",
    "    for j in pair_idx:\n",
    "        if i != j:\n",
    "            cov[i, j] = rho_pair\n",
    "\n",
    "# apply strong block correlation\n",
    "for i in strong_block_idx:\n",
    "    for j in strong_block_idx:\n",
    "        if i != j:\n",
    "            cov[i, j] = rho_block_strong\n",
    "\n",
    "# symmetry and PSD correction\n",
    "cov = (cov + cov.T) / 2.0\n",
    "eigvals = np.linalg.eigvalsh(cov)\n",
    "if eigvals.min() < -1e-8:\n",
    "    jitter = abs(eigvals.min()) + 1e-8\n",
    "    cov += np.eye(p) * jitter\n",
    "\n",
    "# --- latent correlated normals ---\n",
    "mean = np.zeros(p)\n",
    "Z = np.random.multivariate_normal(mean, cov, size=n)\n",
    "\n",
    "# --- transform to desired marginals ---\n",
    "X1 = 4 * (Z[:, 0] - Z[:, 0].min()) / (Z[:, 0].max() - Z[:, 0].min()) - 2   # uniform(-2,2)\n",
    "X2 = np.exp(Z[:, 1])            # exponential-like\n",
    "X3 = (Z[:, 2] > 0).astype(int)  # binary\n",
    "X4 = Z[:, 3]                    # normal\n",
    "X5 = Z[:, 4]\n",
    "U6 = norm.cdf(Z[:, 5])                       # map to uniform(0,1)\n",
    "X6 = poisson.ppf(U6, mu=4).clip(0, 5)        # Poisson quantile\n",
    "X6 = (X6 - X6.mean()) / X6.std()             # normalize to mean 0, sd 1\n",
    "X7 = Z[:, 6]\n",
    "X8 = Z[:, 7]\n",
    "X9  = np.random.gamma(shape=2.0, scale=np.exp(Z[:, 8])/3, size=n)      # gamma, correlated through Z\n",
    "X10 = np.exp(Z[:, 9])                                                # log-normal, correlated through Z\n",
    "X11 = (np.random.pareto(a=2.5, size=n) + 1) * (1 + 0.2 * Z[:, 10])    # weakly correlated Pareto\n",
    "\n",
    "# combine into matrix\n",
    "X = np.column_stack([X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11])\n",
    "\n",
    "# --- true effects (X9 & X10 associated with Y) ---\n",
    "beta = np.array([0.8, 0.8, 0.0, 0.8, 0.8, 0.8, 0.0, 0.0, 0.8, 0.8, 0.0]) #3, 7,8,11 no effect\n",
    "\n",
    "# X1 has sinusoidal effect\n",
    "# X9 has log effect (heavy-tailed variable with diminishing effect)\n",
    "lin_pred = (\n",
    "    1.5 * np.sin(np.pi * X1 / 2) +           # nonlinear in X1\n",
    "    0.8 * np.log1p(X9) +                     # nonlinear in X9\n",
    "    X @ beta                                 # rest linear\n",
    ")\n",
    "p_prob = expit(lin_pred)\n",
    "Y = np.random.binomial(1, p_prob)\n",
    "\n",
    "# --- wrap up in DataFrame ---\n",
    "cols = [f\"X{i+1}\" for i in range(X.shape[1])]\n",
    "df = pd.DataFrame(X, columns=cols)\n",
    "df[\"Y\"] = Y\n",
    "\n",
    "# identify binary vs continuous columns\n",
    "def is_binary_col(col):\n",
    "    u = np.unique(col)\n",
    "    return set(u).issubset({0, 1})\n",
    "\n",
    "types = [\"bin\" if is_binary_col(df[c]) else \"cont\" for c in cols]\n",
    "\n",
    "# --- summaries ---\n",
    "print(\"Correlation matrix (strong block):\")\n",
    "print(df[[\"X4\", \"X5\", \"X6\", \"X9\", \"X10\"]].corr().round(2))\n",
    "\n",
    "print(\"\\nSkewed variable summaries (X9X11):\")\n",
    "print(df[[\"X9\", \"X10\", \"X11\"]].describe().round(2))\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Expected logistic slope\n",
    "avg_slope = np.mean(p * (1 - p))\n",
    "\n",
    "# Compute variance contribution of each term\n",
    "var_contrib = np.zeros(X.shape[1])\n",
    "\n",
    "for j in range(X.shape[1]):\n",
    "    if j == 0:\n",
    "        term = 1.5 * np.sin(np.pi * X1 / 2)\n",
    "    elif j == 8:\n",
    "        term = 0.8 * np.log1p(X9)\n",
    "    else:\n",
    "        term = X[:, j] * beta[j]\n",
    "    var_contrib[j] = np.var(term) * avg_slope\n",
    "\n",
    "df_contrib = pd.DataFrame({\n",
    "    \"variable\": [f\"X{i+1}\" for i in range(11)],\n",
    "    \"var_contrib\": var_contrib\n",
    "})\n",
    "\n",
    "df_contrib = df_contrib.sort_values(\"var_contrib\", ascending=True).reset_index(drop=True)\n",
    "print(df_contrib)\n",
    "\n",
    "X = pd.DataFrame(X, columns=cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df778ad-af7c-452d-a7c1-61492b48d834",
   "metadata": {},
   "source": [
    "### KNN Knockoff P Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b7a366-a952-4ed4-9f25-07271bb4fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "knockoff_knn = construct_knockoffs_knn(X, types, nbrs_per_sample=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3fd068-add4-41b8-a4be-36385b8cc563",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_knn = permutation_pvalues(X, knockoff_knn, Y, B=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6082dbf-828d-4112-b3b4-33a370a55c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reject, pvals_corrected, _, _ = multipletests(perm_knn['p_value'], method='fdr_bh')\n",
    "\n",
    "# add the adjusted p-values to DF\n",
    "perm_knn['p_value_bh'] = pvals_corrected\n",
    "\n",
    "# sort by the adjusted p-values\n",
    "perm_knn_sorted = perm_knn.sort_values('p_value_bh').reset_index(drop=True)\n",
    "\n",
    "perm_knn_sorted.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c26845-9a67-489b-b802-02345f222c46",
   "metadata": {},
   "source": [
    "### Gaussian Knockoff P Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d266ce80-52b3-4a37-908a-f5e08ed00019",
   "metadata": {},
   "outputs": [],
   "source": [
    "knockoff_gauss = gaussian_knockoff(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a7038-7fce-4a6e-a97e-3d9aaa500c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_gauss = permutation_pvalues(X, knockoff_gauss, Y, B=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c79a45-5e7d-41b1-b9ee-0b368817537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reject, pvals_corrected, _, _ = multipletests(perm_gauss['p_value'], method='fdr_bh')\n",
    "\n",
    "# add the adjusted p-values to DF\n",
    "perm_gauss['p_value_bh'] = pvals_corrected\n",
    "\n",
    "# sort by the adjusted p-values\n",
    "perm_gauss_sorted = perm_gauss.sort_values('p_value_bh').reset_index(drop=True)\n",
    "\n",
    "perm_gauss_sorted.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125c5998-8107-4944-935b-389cb800eeb9",
   "metadata": {},
   "source": [
    "### Deep Knockoff P Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b5716d-afe3-4cb1-9bf3-fad54873414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = X.shape[1]\n",
    "qt = QuantileTransformer(output_distribution='normal')\n",
    "X_scaled = qt.fit_transform(X)\n",
    "X_scaled = np.array(X_scaled)\n",
    "SigmaHat = np.cov(X_scaled, rowvar=False)\n",
    "second_order = GaussianKnockoffs(SigmaHat, mu=np.mean(X_scaled,0), method=\"sdp\")\n",
    "corr_g = (np.diag(SigmaHat) - np.diag(second_order.Ds)) / np.diag(SigmaHat)\n",
    "\n",
    "# Set the parameters for training deep knockoffs\n",
    "pars = dict()\n",
    "# Number of epochs\n",
    "pars['epochs'] = 100\n",
    "# Number of iterations over the full data per epoch\n",
    "pars['epoch_length'] = 50\n",
    "# Data type, either \"continuous\" or \"binary\"\n",
    "pars['family'] = \"continuous\"\n",
    "# Dimensions of the data\n",
    "pars['p'] = p\n",
    "# Size of the test set\n",
    "pars['test_size']  = int(0.1*n)\n",
    "# Batch size\n",
    "pars['batch_size'] = int(0.45*n)\n",
    "# Learning rate\n",
    "pars['lr'] = 0.003\n",
    "# When to decrease learning rate (unused when equal to number of epochs)\n",
    "pars['lr_milestones'] = [pars['epochs']]\n",
    "# Width of the network (number of layers is fixed to 6)\n",
    "pars['dim_h'] = int(10*p)\n",
    "# Penalty for the MMD distance\n",
    "pars['GAMMA'] = 0.5\n",
    "# Penalty encouraging second-order knockoffs\n",
    "pars['LAMBDA'] = 0.5\n",
    "# Decorrelation penalty hyperparameter\n",
    "pars['DELTA'] = 0.5\n",
    "# Target pairwise correlations between variables and knockoffs\n",
    "pars['target_corr'] = corr_g\n",
    "# Kernel widths for the MMD measure (uniform weights)\n",
    "pars['alphas'] = [1.,2.,4.,8.,16.,32.,64.,128.]\n",
    "\n",
    "# Initialize the machine\n",
    "machine = KnockoffMachine(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac004d8c-b472-442e-b2d9-f777fa749cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fitting the knockoff machine...\")\n",
    "machine.train(X_scaled)\n",
    "X_deepknockoff = machine.generate(X_scaled)\n",
    "print(\"Size of the deep knockoff dataset: %d x %d.\" %(X_deepknockoff.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b96abea-6d89-4f34-b375-bca85d5b268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_deep = permutation_pvalues(X_scaled, X_deepknockoff, Y, B=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56db57ff-c297-4719-a0b3-92259236da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reject, pvals_corrected, _, _ = multipletests(perm_deep['p_value'], method='fdr_bh')\n",
    "\n",
    "# add the adjusted p-values to DF\n",
    "perm_deep['p_value_bh'] = pvals_corrected\n",
    "\n",
    "# sort by the adjusted p-values\n",
    "perm_deep_sorted = perm_deep.sort_values('p_value_bh').reset_index(drop=True)\n",
    "\n",
    "perm_deep_sorted.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca52989-4325-4f8a-9d36-c8dc45d86e31",
   "metadata": {},
   "source": [
    "### Permutation Only - No Knockoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b396de0-27f5-40d8-a427-cd105a739128",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_only = permutation_mda_pvalues(X,Y, B=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2492c-c025-4b0c-a97d-ff3dc8f44785",
   "metadata": {},
   "outputs": [],
   "source": [
    "reject, pvals_corrected, _, _ = multipletests(perm_only['p_value'], method='fdr_bh')\n",
    "\n",
    "# add the adjusted p-values to DF\n",
    "perm_only['p_value_bh'] = pvals_corrected\n",
    "\n",
    "# sort by the adjusted p-values\n",
    "perm_only_sorted = perm_only.sort_values('p_value_bh').reset_index(drop=True)\n",
    "\n",
    "perm_only_sorted.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a7c4e8-bffb-4fa6-9894-5ecfec721d7e",
   "metadata": {},
   "source": [
    "## Concussion Data Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901f7a34-65ba-4733-a1ee-4e9494a31666",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04c7fbf2-6e40-46de-b5ed-4149bba35dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_id                     0\n",
      "age                           0\n",
      "sex                           0\n",
      "race                          0\n",
      "ethnicity                     0\n",
      "                             ..\n",
      "gad_7_7                       0\n",
      "gad7_total_score              0\n",
      "gad7_difficult_to_function    0\n",
      "psqi                          0\n",
      "time_since_injury             0\n",
      "Length: 77, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV\n",
    "conc_data = pd.read_csv(\"ContexData_TimeToClinicLessThan2Weeks.csv\")\n",
    "\n",
    "# Function to detect binary\n",
    "def is_binary_col(col: np.ndarray, tol=1e-8):\n",
    "    uniques = np.unique(col[~np.isnan(col)])\n",
    "    return set(uniques).issubset({0, 1}) and len(uniques) <= 2\n",
    "\n",
    "# Make a copy\n",
    "conc_data_imputed = conc_data.copy()\n",
    "conc_data_imputed = conc_data_imputed.iloc[:,2:]\n",
    "\n",
    "imputer = IterativeImputer(random_state=0, max_iter=10, sample_posterior=True)\n",
    "\n",
    "# Fit and transform the data\n",
    "conc_data_imputed[:] = imputer.fit_transform(conc_data_imputed)\n",
    "\n",
    "# Check missingness\n",
    "print(conc_data_imputed.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d818af7f-7b36-46ec-b0f8-11d5b9f8c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = conc_data_imputed\n",
    "X = X.loc[:, X.nunique() > 1]\n",
    "# Automatically detect types\n",
    "types = []\n",
    "for col_name in X.columns:\n",
    "    col = X[col_name].values\n",
    "    if is_binary_col(col):\n",
    "        types.append(\"bin\")\n",
    "    else:\n",
    "        types.append(\"cont\")\n",
    "y = (conc_data.iloc[:, 1] <= 3).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ec2cba5-28a1-4c90-9442-0ce7f7853d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e482e34-65ab-45b0-962e-6fcc22c5137f",
   "metadata": {},
   "source": [
    "### KNN Knockoff P Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e78c3cde-36e2-4ea4-bf80-16f3b0c94126",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_cols = [c for c, t in zip(X.columns, types) if t == \"cont\"]\n",
    "bin_cols  = [c for c, t in zip(X.columns, types) if t == \"bin\"]\n",
    "\n",
    "X_scaled = X.copy()\n",
    "\n",
    "qt = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "if cont_cols:  # avoid empty slice\n",
    "    X_scaled[cont_cols] = qt.fit_transform(X_scaled[cont_cols])\n",
    "\n",
    "# Ensure binary columns remain as 0/1 integers\n",
    "X_scaled[bin_cols] = X_scaled[bin_cols].round().astype(int)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=cols)\n",
    "knockoff_knn = construct_knockoffs_knn(X_scaled, types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82e2ae6e-e55c-4508-875b-ee3f6d30d1a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'permutation_pvalues' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m perm_knn = \u001b[43mpermutation_pvalues\u001b[49m(X, knockoff_knn, y, B=\u001b[32m300\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'permutation_pvalues' is not defined"
     ]
    }
   ],
   "source": [
    "perm_knn = permutation_pvalues(X, knockoff_knn, y, B=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a1129c-a443-4b45-8fa5-a490c940d0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reject, pvals_corrected, _, _ = multipletests(perm_knn['p_value'], method='fdr_bh')\n",
    "\n",
    "# add the adjusted p-values DF\n",
    "perm_knn['p_value_bh'] = pvals_corrected\n",
    "\n",
    "# sort by the adjusted p-values\n",
    "perm_knn_sorted = perm_knn.sort_values('p_value_bh').reset_index(drop=True)\n",
    "\n",
    "perm_knn_sorted.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549760fb-96d6-49f9-bc12-f98a0896259b",
   "metadata": {},
   "source": [
    "### Gaussian Knockoff P Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e2df6-b5e7-45de-995f-c4ff86e9b275",
   "metadata": {},
   "outputs": [],
   "source": [
    "knockoff_gauss = gaussian_knockoff(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4d5167-fbeb-4892-a2fa-2bac83829201",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_gauss = permutation_pvalues(X, knockoff_gauss, y, B=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd89b020-cc1a-4d1d-a3a9-79d25ff75668",
   "metadata": {},
   "outputs": [],
   "source": [
    "reject, pvals_corrected, _, _ = multipletests(perm_gauss['p_value'], method='fdr_bh')\n",
    "\n",
    "# add the adjusted p-values to DF\n",
    "perm_gauss['p_value_bh'] = pvals_corrected\n",
    "\n",
    "# sort by the adjusted p-values\n",
    "perm_gauss_sorted = perm_gauss.sort_values('p_value_bh').reset_index(drop=True)\n",
    "\n",
    "perm_gauss_sorted.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed0b06c-a34c-4bc5-8640-aa4bbb01117b",
   "metadata": {},
   "source": [
    "### Deep Knockoff P Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f46013d-20be-4269-8d7f-cae1c52fc63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = X.shape[1]\n",
    "\n",
    "qt = QuantileTransformer(output_distribution='normal')\n",
    "X_scaled = qt.fit_transform(X)\n",
    "X_scaled = np.array(X_scaled)\n",
    "SigmaHat = np.cov(X_scaled, rowvar=False)\n",
    "SigmaHat += 1e-1 * np.eye(SigmaHat.shape[0])  # a bit stronger ridge for stability\n",
    "def make_posdef(Sigma, eps=1e-3):\n",
    "    # Symmetrize\n",
    "    Sigma = (Sigma + Sigma.T) / 2\n",
    "    # Eigen-decompose\n",
    "    eigvals, eigvecs = np.linalg.eigh(Sigma)\n",
    "    # Clip eigenvalues to be at least eps\n",
    "    eigvals_clipped = np.clip(eigvals, eps, None)\n",
    "    return eigvecs @ np.diag(eigvals_clipped) @ eigvecs.T\n",
    "\n",
    "SigmaHat = make_posdef(SigmaHat, eps=1e-3)\n",
    "second_order = GaussianKnockoffs(SigmaHat, mu=np.mean(X_scaled,0), method=\"equi\")\n",
    "# Compute correlation targets safely\n",
    "corr_g = (np.diag(SigmaHat) - np.diag(second_order.Ds)) / (np.diag(SigmaHat) + 1e-8)\n",
    "corr_g = np.clip(corr_g, 0.1, 0.4)  # a little easier target\n",
    "corr_g += np.random.uniform(-0.05, 0.05, size=corr_g.shape)\n",
    "# Set the parameters for training deep knockoffs\n",
    "pars = dict()\n",
    "# Number of epochs\n",
    "pars['epochs'] = 100\n",
    "# Number of iterations over the full data per epoch\n",
    "pars['epoch_length'] = 50\n",
    "# Data type, either \"continuous\" or \"binary\"\n",
    "pars['family'] = \"continuous\"\n",
    "# Dimensions of the data\n",
    "pars['p'] = p\n",
    "# Size of the test set\n",
    "pars['test_size']  = int(0.1*n)\n",
    "# Batch size\n",
    "pars['batch_size'] = int(0.3*n)\n",
    "# Learning rate\n",
    "pars['lr'] = 0.003\n",
    "# When to decrease learning rate (unused when equal to number of epochs)\n",
    "pars['lr_milestones'] = [pars['epochs']]\n",
    "# Width of the network (number of layers is fixed to 6)\n",
    "pars['dim_h'] = int(3*p)\n",
    "# Penalty for the MMD distance\n",
    "pars['GAMMA'] = 0.5\n",
    "# Penalty encouraging second-order knockoffs\n",
    "pars['LAMBDA'] = 1\n",
    "# Decorrelation penalty hyperparameter\n",
    "pars['DELTA'] = 1\n",
    "# Target pairwise correlations between variables and knockoffs\n",
    "pars['target_corr'] = corr_g\n",
    "# Kernel widths for the MMD measure (uniform weights)\n",
    "pars['alphas'] = [1.,2.,4.,8.,16.,32.,64.,128.]\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
    "\n",
    "\n",
    "# Initialize the machine\n",
    "machine = KnockoffMachine(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d10064b-82de-48ab-907d-b85024e3d9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"corr_g min:\", np.min(corr_g))\n",
    "print(\"corr_g max:\", np.max(corr_g))\n",
    "print(\"Any NaNs:\", np.any(~np.isfinite(corr_g)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3544379-6eaf-4448-8c2c-923b9d465339",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fitting the knockoff machine...\")\n",
    "machine.train(X_scaled)\n",
    "X_deepknockoff = machine.generate(X_scaled)\n",
    "print(\"Size of the deep knockoff dataset: %d x %d.\" %(X_deepknockoff.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a460d14-d3a2-4393-9fa9-69c56dd3aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = pd.DataFrame(X_scaled, columns=cols)\n",
    "X_deepknockoff = pd.DataFrame(X_deepknockoff, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bce65b-f8b0-4ab0-9a67-df9ded143a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_deep = permutation_pvalues(X, X_deepknockoff, y, B=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5220bf2c-60b6-4bfd-8a3b-93635b9c4a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "reject, pvals_corrected, _, _ = multipletests(perm_deep['p_value'], method='fdr_bh')\n",
    "\n",
    "# add the adjusted p-values to DF\n",
    "perm_deep['p_value_bh'] = pvals_corrected\n",
    "\n",
    "# sort by the adjusted p-values\n",
    "perm_deep_sorted = perm_deep.sort_values('p_value_bh').reset_index(drop=True)\n",
    "#perm_deep_sorted['variable_name'] = perm_deep_sorted['variable'].apply(lambda x: cols[x])\n",
    "\n",
    "perm_deep_sorted.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0d9d80-b6ad-4e28-b103-0dd0158ad36a",
   "metadata": {},
   "source": [
    "### Permutation Only - No Knockoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bddc21b-a8dc-47a5-bb6d-fe47adfe0923",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_only = permutation_mda_pvalues(X,y, B=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87c9f4c-d205-4018-a8ad-3f1591a34694",
   "metadata": {},
   "outputs": [],
   "source": [
    "reject, pvals_corrected, _, _ = multipletests(perm_only['p_value'], method='fdr_bh')\n",
    "\n",
    "# add the adjusted p-values to DF\n",
    "perm_only['p_value_bh'] = pvals_corrected\n",
    "\n",
    "# sort by the adjusted p-values\n",
    "perm_only_sorted = perm_only.sort_values('p_value_bh').reset_index(drop=True)\n",
    "\n",
    "perm_only_sorted.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0570c6d7-bc5c-49fa-b430-ea2f24c314a8",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d4ed24-481d-4114-8186-9cd743f23b5c",
   "metadata": {},
   "source": [
    "Sesia, M. (n.d.). DeepKnockoffs [Python package]. GitHub. https://github.com/msesia/deepknockoffs/tree/master/DeepKnockoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19ef595-6451-4a73-88c9-a63723c2ed67",
   "metadata": {},
   "source": [
    "Romano, Y., Sesia, M., & Cands, E. (2020). Deep knockoffs. Journal of the American Statistical Association, 115(532), 1861-1872. https://doi.org/10.1080/01621459.2019.1660174"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
